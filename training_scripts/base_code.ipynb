{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725fa18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Fine-Tuning LLaVA 1.5 7B (HF version) for Instagram Captioning on Custom JSONL Data\n",
    "# Compatible with RTX 3060 / T4 (12-16 GB GPUs)\n",
    "\n",
    "!pip install -U \"transformers>=4.39.0\"\n",
    "!pip install peft bitsandbytes\n",
    "!pip install -U \"trl>=0.8.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d999c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01df042",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f0617",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "BASE_IMAGES_DIR = \"/mnt/InstaCities1M/img_resized_1M/cities_instagram/\"\n",
    "BASE_CAPTIONS_DIR = \"/mnt/InstaCities1M/captions_resized_1M/cities_instagram/\"\n",
    "OUTPUT_JSONL_PATH = './datasset_v1.jsonl'\n",
    "cities = ['newyork']\n",
    "\n",
    "LLAVA_CHAT_TEMPLATE = (\n",
    "    \"You are a social media influencer. Write a captivating Instagram caption for this image \"\n",
    "    \"that will engage more viewers and boost interaction. Analyze the image to decide the tone of the caption.\"\n",
    ")\n",
    "\n",
    "print(os.path.exists(BASE_IMAGES_DIR))\n",
    "print(os.path.exists(BASE_CAPTIONS_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5015894",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "images_files = []\n",
    "captions_files = []\n",
    "\n",
    "for city in cities:\n",
    "    img = BASE_IMAGES_DIR + city + '/' + np.array(os.listdir(BASE_IMAGES_DIR + city))\n",
    "    caption = BASE_CAPTIONS_DIR + city + '/' + np.array(os.listdir(BASE_CAPTIONS_DIR + city))\n",
    "    images_files.extend(img)\n",
    "    captions_files.extend(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5557a8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Clean filenames\n",
    "image_ids = {os.path.splitext(os.path.basename(img))[0] for img in images_files}\n",
    "caption_ids = {os.path.splitext(os.path.basename(cap))[0] for cap in captions_files}\n",
    "\n",
    "# Now match\n",
    "common_ids = image_ids & caption_ids\n",
    "\n",
    "# Filter\n",
    "filtered_image_files = [img for img in images_files if os.path.splitext(os.path.basename(img))[0] in common_ids]\n",
    "filtered_caption_files = [cap for cap in captions_files if os.path.splitext(os.path.basename(cap))[0] in common_ids]\n",
    "\n",
    "images_files = filtered_image_files\n",
    "captions_files = filtered_caption_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3bd892",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "len(images_files), len(captions_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0f856",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# # --- Worker function ---\n",
    "# def process_pair(i):\n",
    "#     try:\n",
    "#         img_path = images_files[i]\n",
    "#         caption_path = captions_files[i]\n",
    "\n",
    "#         with open(caption_path, 'r', encoding='utf-8') as f:\n",
    "#             caption = f.read().strip().replace('\\n', ' ')\n",
    "#             if not caption:\n",
    "#                 return None\n",
    "\n",
    "#         return {\n",
    "#             \"image_path\": img_path,\n",
    "#             \"prompt\": PROMPT_TEMPLATE,\n",
    "#             \"response\": caption\n",
    "#         }\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "# --- Worker function ---\n",
    "def process_pair(i):\n",
    "    try:\n",
    "        img_path = images_files[i]\n",
    "        caption_path = captions_files[i]\n",
    "\n",
    "        with open(caption_path, 'r', encoding='utf-8') as f:\n",
    "            caption = f.read().strip().replace('\\n', ' ')\n",
    "            if not caption:\n",
    "                return None\n",
    "\n",
    "        # Create messages field directly\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": LLAVA_CHAT_TEMPLATE},\n",
    "                    {\"type\": \"image\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": caption}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"image_path\": img_path,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Multiprocessing ---\n",
    "with Pool(cpu_count()) as pool:\n",
    "    results = list(tqdm(pool.imap(process_pair, range(len(images_files))), total=len(images_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243917a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = [entry for entry in results if entry is not None]\n",
    "\n",
    "# --- Write JSONL File ---\n",
    "with open(OUTPUT_JSONL_PATH, 'w', encoding='utf-8') as f:\n",
    "    for entry in data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"JSONL created: {OUTPUT_JSONL_PATH} with {len(data)} samples.\")\n",
    "PROMPT_TEMPLATE = \"Write an Instagram caption for this image to be posted:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fb514",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "data_path = OUTPUT_JSONL_PATH  # path to your formatted JSONL file\n",
    "output_dir = \"./llava_lora_instagram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7822cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Model Loading (4bit Quantization) ---\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "tokenizer.chat_template = (\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if message['role'] == 'user' %}USER: {% else %}ASSISTANT: {% endif %}\"\n",
    "    \"{% for item in message['content'] %}\"\n",
    "    \"{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}{{ eos_token }}{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2287eae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_image(example):\n",
    "    example[\"image\"] = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "    return example\n",
    "\n",
    "# Load and preprocess dataset with pre-formatted messages\n",
    "dataset = load_dataset(\"json\", data_files=data_path)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de86fe7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa1b360",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read JSONL manually\n",
    "dataset = []\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line.strip())\n",
    "        dataset.append(example)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples.\")\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1992f23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Data Collator ---\n",
    "# class LLavaDataCollator:\n",
    "#     def __init__(self, processor):\n",
    "#         self.processor = processor\n",
    "\n",
    "#     def __call__(self, examples):\n",
    "#         texts = []\n",
    "#         images = []\n",
    "#         for example in examples:\n",
    "#             messages = example[\"messages\"]\n",
    "#             text = self.processor.tokenizer.apply_chat_template(\n",
    "#                 messages, tokenize=False, add_generation_prompt=False\n",
    "#             )\n",
    "#             texts.append(text)\n",
    "             \n",
    "#             # Load image dynamically during batching\n",
    "#             image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "#             images.append(image)\n",
    "            \n",
    "#         batch = self.processor(texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "#         labels = batch[\"input_ids\"].clone()\n",
    "#         if self.processor.tokenizer.pad_token_id is not None:\n",
    "#             labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "#         batch[\"labels\"] = labels\n",
    "#         return batch\n",
    "\n",
    "# class LLavaDataCollator:\n",
    "#     def __init__(self, processor):\n",
    "#         self.processor = processor\n",
    "\n",
    "#     def __call__(self, examples):\n",
    "#         texts = []\n",
    "#         images = []\n",
    "        \n",
    "#         # Fix: Reconstruct example as dict if needed\n",
    "#         if isinstance(examples[0], dict):\n",
    "#             batch = examples\n",
    "#         else:\n",
    "#             batch = [\n",
    "#                 {k: v for k, v in zip(self.processor.tokenizer.model_input_names, example)}\n",
    "#                 for example in examples\n",
    "#             ]\n",
    "        \n",
    "#         for example in batch:\n",
    "#             messages = example[\"messages\"]\n",
    "\n",
    "#             text = self.processor.tokenizer.apply_chat_template(\n",
    "#                 messages, tokenize=False, add_generation_prompt=False\n",
    "#             )\n",
    "#             texts.append(text)\n",
    "\n",
    "#             image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "#             images.append(image)\n",
    "\n",
    "#         batch = self.processor(texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "#         labels = batch[\"input_ids\"].clone()\n",
    "#         if self.processor.tokenizer.pad_token_id is not None:\n",
    "#             labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "#         batch[\"labels\"] = labels\n",
    "#         return batch\n",
    "\n",
    "# class LLavaDataCollator:\n",
    "#     def __init__(self, processor, dataset_features):\n",
    "#         self.processor = processor\n",
    "#         self.dataset_features = dataset_features\n",
    "\n",
    "#     def __call__(self, examples):\n",
    "#         texts = []\n",
    "#         images = []\n",
    "\n",
    "#         for example in examples:\n",
    "#             if isinstance(example, dict):\n",
    "#                 ex = example\n",
    "#             else:\n",
    "#                 ex = {k: v for k, v in zip(self.dataset_features.keys(), example)}\n",
    "\n",
    "#             messages = ex[\"messages\"]\n",
    "\n",
    "#             text = self.processor.tokenizer.apply_chat_template(\n",
    "#                 messages, tokenize=False, add_generation_prompt=False\n",
    "#             )\n",
    "#             texts.append(text)\n",
    "\n",
    "#             image = Image.open(ex[\"image_path\"]).convert(\"RGB\")\n",
    "#             images.append(image)\n",
    "\n",
    "#         batch = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "#         labels = batch[\"input_ids\"].clone()\n",
    "#         if self.processor.tokenizer.pad_token_id is not None:\n",
    "#             labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "#         batch[\"labels\"] = labels\n",
    "#         return batch\n",
    "\n",
    "# class LLavaDataCollator:\n",
    "#     def __init__(self, processor):\n",
    "#         self.processor = processor\n",
    "\n",
    "#     def __call__(self, examples):\n",
    "#         texts = []\n",
    "#         images = []\n",
    "        \n",
    "#         for example in examples:\n",
    "#             messages = example[\"messages\"]\n",
    "\n",
    "#             text = self.processor.tokenizer.apply_chat_template(\n",
    "#                 messages, tokenize=False, add_generation_prompt=False\n",
    "#             )\n",
    "#             texts.append(text)\n",
    "\n",
    "#             image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "#             images.append(image)\n",
    "\n",
    "#         batch = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "#         labels = batch[\"input_ids\"].clone()\n",
    "#         if self.processor.tokenizer.pad_token_id is not None:\n",
    "#             labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "#         batch[\"labels\"] = labels\n",
    "#         return batch\n",
    "\n",
    "class LLavaDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # Add this print when collator is called\n",
    "        print(\"=\"*50)\n",
    "        print(\"Batch samples received by collator:\")\n",
    "        print(examples)\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Now continue your normal logic\n",
    "        texts = []\n",
    "        images = []\n",
    "        \n",
    "        for example in examples:\n",
    "            messages = example[\"messages\"]\n",
    "\n",
    "            text = self.processor.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(text)\n",
    "\n",
    "            image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "            images.append(image)\n",
    "\n",
    "        batch = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        if self.processor.tokenizer.pad_token_id is not None:\n",
    "            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a6be7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- LoRA Configuration ---\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# --- SFT Trainer ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = LLavaDataCollator(processor\n",
    "                                  # , dataset.features\n",
    "                                 )\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Wrap\n",
    "train_dataset = SimpleDataset(dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    processing_class =tokenizer,\n",
    "    # data_collator=data_collator,\n",
    "    peft_config=lora_config,\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf3739",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Start Fine-tuning ---\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9fd507",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Save Final Model ---\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "print(f\"âœ… Training complete. Model saved at {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b53a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('llava_lora_instagram', 'zip', 'llava_lora_instagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424bac4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Inference function\n",
    "def generate_caption(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Prepare the input prompt (same as training)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a social media influencer. Write a captivating Instagram caption for this image that will engage more viewers and boost interaction. Analyze the image to decide the tone of the caption.\"},\n",
    "            {\"type\": \"image\"}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    input_tensors = processor(text=inputs, images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        # output = model.generate(**input_tensors, max_new_tokens=80)\n",
    "        output = model.generate(\n",
    "            **input_tensors,\n",
    "            max_new_tokens=80,\n",
    "            repetition_penalty=1.2,   # Encourage less repetition\n",
    "            temperature=0.7,          # Add some randomness\n",
    "            top_p=0.9,                # Top-p sampling (nucleus sampling)\n",
    "            do_sample=True            # Enable sampling instead of greedy decoding\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = processor.batch_decode(output[:, input_tensors[\"input_ids\"].shape[1]:], skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957116c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./temp.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c4326",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test1.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64805ab6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test3.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a9ea5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test4.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ee258",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test5.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca20ec97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test6.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a216b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test7.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986878e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b5fbd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4ef9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# def chat_with_model(messages, image=None):\n",
    "#     \"\"\"\n",
    "#     Function to send messages to the model and get a reply.\n",
    "#     - `messages`: current conversation list\n",
    "#     - `image`: PIL.Image if needed for the first user input\n",
    "#     \"\"\"\n",
    "#     # Prepare input\n",
    "#     if image:\n",
    "#         inputs = processor.apply_chat_template(messages, images=[image], return_tensors=\"pt\", tokenize=True, add_generation_prompt=True)\n",
    "#     else:\n",
    "#         inputs = processor.apply_chat_template(messages, return_tensors=\"pt\", tokenize=True, add_generation_prompt=True)\n",
    "    \n",
    "#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "#     # Generate\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=100,\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.9,\n",
    "#             repetition_penalty=1.1,\n",
    "#             do_sample=True\n",
    "#         )\n",
    "    \n",
    "#     # Decode output\n",
    "#     reply = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "#     return reply\n",
    "\n",
    "def chat_with_model(messages, image=None):\n",
    "    \"\"\"\n",
    "    Function to send messages to the model and get a reply.\n",
    "    \"\"\"\n",
    "    # Step 1: Create chat template\n",
    "    prompt_text = processor.tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: Encode inputs\n",
    "    if image:\n",
    "        inputs = processor(text=prompt_text, images=[image], return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        inputs = processor(text=prompt_text, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Step 3: Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # Step 4: Decode output\n",
    "    reply = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9291f40d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------\n",
    "# Start a conversation\n",
    "# ------------------\n",
    "\n",
    "# Step 1: Initial messages with an image\n",
    "image_path = \"test2.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a social media influencer. Write a catchy Instagram caption for this image.\"},\n",
    "            {\"type\": \"image\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# First model reply\n",
    "caption = chat_with_model(messages, image=image)\n",
    "print(f\"\\nðŸ§  Model: {caption}\")\n",
    "\n",
    "# Add model's response to messages\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\"type\": \"text\", \"text\": caption}]\n",
    "})\n",
    "\n",
    "# Step 2: Loop for continuous chat\n",
    "while True:\n",
    "    user_input = input(\"\\nðŸ’¬ Your input (type 'quit' to stop): \")\n",
    "    \n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"ðŸ‘‹ Ending chat. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    # Add user's new message\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": user_input}]\n",
    "    })\n",
    "    \n",
    "    # Get model's reply\n",
    "    model_reply = chat_with_model(messages)\n",
    "    print(f\"\\nðŸ§  Model: {model_reply}\")\n",
    "    \n",
    "    # Add model's reply back to messages\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": model_reply}]\n",
    "    })\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
