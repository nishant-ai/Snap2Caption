{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03674a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fine-Tuning LLaVA 1.5 7B (HF version) for Instagram Captioning on Custom JSONL Data\n",
    "# # Compatible with RTX 3060 / T4 (12-16 GB GPUs)\n",
    "\n",
    "# !pip install -U \"transformers>=4.39.0\"\n",
    "# !pip install peft bitsandbytes\n",
    "# !pip install -U \"trl>=0.8.3\"\n",
    "# !pip install wandb\n",
    "# !pip install evaluate\n",
    "# !pip install datasets\n",
    "# !pip install rouge_score\n",
    "# !pip install torchao\n",
    "# # !pip install git+https://github.com/Maluuba/nlg-eval.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0c8ab",
   "metadata": {},
   "source": [
    "Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d77aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import torchao\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from torch.optim import AdamW\n",
    "# from datasets import load_metric\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from evaluate import load as load_metric\n",
    "from datasets import Dataset as HFDataset\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05374b",
   "metadata": {},
   "source": [
    "Setting Up WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"d707117fb8a8f4cf9916b4cf42fe630e09c93b6b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b11af11",
   "metadata": {},
   "source": [
    "Setting Up Script Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data configs\n",
    "DATASET = \"InstaCities1M\"\n",
    "BASE_IMAGES_DIR = \"/mnt/InstaCities1M/img_resized_1M/cities_instagram/\"\n",
    "BASE_CAPTIONS_DIR = \"/mnt/InstaCities1M/captions_resized_1M/cities_instagram/\"\n",
    "OUTPUT_JSONL_PATH = './datasset_v1.jsonl'\n",
    "CITIES = ['newyork']\n",
    "\n",
    "LLAVA_CHAT_TEMPLATE = (\n",
    "    \"You are a social media influencer. Write a captivating Instagram caption for this image \"\n",
    "    \"that will engage more viewers and boost interaction. Analyze the image to decide the tone of the caption.\"\n",
    ")\n",
    "\n",
    "print(os.path.exists(BASE_IMAGES_DIR))\n",
    "print(os.path.exists(BASE_CAPTIONS_DIR))\n",
    "\n",
    "# Expirementation Details\n",
    "PROJECT = \"Snap2Caption\"\n",
    "RUN_NAME = \"llava-7b-ft-instagram-v1\"\n",
    "\n",
    "# model configs\n",
    "MODEL_NAME = \"LLaVA-7B-HF (LLaVA-1.5-7B)\"\n",
    "TASK = \"Image Captioning\"\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "# Search/Filter Tags\n",
    "TAGS = [\"llava\", \"image-captioning\", \"LoRA\", \"fine-tuning\"]\n",
    "GROUP = \"llava-instagram-experiments\"\n",
    "NOTES = \"Baseline fine-tuning on InstaCities1M with 4-bit quantized model and LoRA adapters.\"\n",
    "\n",
    "# Transformer Setting\n",
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "MODEL_SAVE_PATH = \"./llava_lora_instagram\"\n",
    "\n",
    "# Optimization Strategy\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "NO_OF_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "LOGGING_STEPS = 100\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4162bb",
   "metadata": {},
   "source": [
    "Verifying GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c9162",
   "metadata": {},
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb426d",
   "metadata": {},
   "source": [
    "Dataset Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f9c23",
   "metadata": {},
   "source": [
    "images_files = []\n",
    "captions_files = []\n",
    "\n",
    "for city in CITIES:\n",
    "    img = BASE_IMAGES_DIR + city + '/' + np.array(os.listdir(BASE_IMAGES_DIR + city))\n",
    "    caption = BASE_CAPTIONS_DIR + city + '/' + np.array(os.listdir(BASE_CAPTIONS_DIR + city))\n",
    "    images_files.extend(img)\n",
    "    captions_files.extend(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4827eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean filenames\n",
    "image_ids = {os.path.splitext(os.path.basename(img))[0] for img in images_files}\n",
    "caption_ids = {os.path.splitext(os.path.basename(cap))[0] for cap in captions_files}\n",
    "\n",
    "# Now match\n",
    "common_ids = image_ids & caption_ids\n",
    "\n",
    "# Filter\n",
    "filtered_image_files = [img for img in images_files if os.path.splitext(os.path.basename(img))[0] in common_ids]\n",
    "filtered_caption_files = [cap for cap in captions_files if os.path.splitext(os.path.basename(cap))[0] in common_ids]\n",
    "\n",
    "images_files = filtered_image_files\n",
    "captions_files = filtered_caption_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_files), len(captions_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85061bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Worker function ---\n",
    "def process_pair(i):\n",
    "    try:\n",
    "        img_path = images_files[i]\n",
    "        caption_path = captions_files[i]\n",
    "\n",
    "        with open(caption_path, 'r', encoding='utf-8') as f:\n",
    "            caption = f.read().strip().replace('\\n', ' ')\n",
    "            if not caption:\n",
    "                return None\n",
    "\n",
    "        # Create messages field directly\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": LLAVA_CHAT_TEMPLATE},\n",
    "                    {\"type\": \"image\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": caption}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"image_path\": img_path,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Multiprocessing ---\n",
    "with Pool(cpu_count()) as pool:\n",
    "    results = list(tqdm(pool.imap(process_pair, range(len(images_files))), total=len(images_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2097a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [entry for entry in results if entry is not None]\n",
    "\n",
    "# --- Write JSONL File ---\n",
    "with open(OUTPUT_JSONL_PATH, 'w', encoding='utf-8') as f:\n",
    "    for entry in data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"JSONL created: {OUTPUT_JSONL_PATH} with {len(data)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0fbf6",
   "metadata": {},
   "source": [
    "Configuring Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b207172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "model_id = MODEL_ID\n",
    "data_path = OUTPUT_JSONL_PATH  # path to your formatted JSONL file\n",
    "output_dir = MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6363d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Loading (4bit Quantization) ---\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "tokenizer.chat_template = (\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if message['role'] == 'user' %}USER: {% else %}ASSISTANT: {% endif %}\"\n",
    "    \"{% for item in message['content'] %}\"\n",
    "    \"{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}{{ eos_token }}{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd12c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(example):\n",
    "    example[\"image\"] = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "    return example\n",
    "\n",
    "# Load and preprocess dataset with pre-formatted messages\n",
    "dataset = load_dataset(\"json\", data_files=data_path)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcef392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSONL manually\n",
    "dataset = []\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line.strip())\n",
    "        dataset.append(example)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples.\")\n",
    "\n",
    "dataset = HFDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Wrap\n",
    "train_dataset = SimpleDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load_metric(\"bleu\")\n",
    "rouge = load_metric(\"rouge\")\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    preds = preds.argmax(dim=-1) if hasattr(preds, \"argmax\") else preds\n",
    "    preds = preds.tolist()\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    pred_texts = [text.strip().split() for text in pred_texts]\n",
    "    label_texts = [[text.strip().split()] for text in label_texts]\n",
    "\n",
    "    bleu_score = bleu.compute(predictions=pred_texts, references=label_texts)[\"bleu\"]\n",
    "    rouge_score = rouge.compute(predictions=[\" \".join(p) for p in pred_texts], references=[\" \".join(l[0]) for l in label_texts])[\"rougeL\"]\n",
    "    \n",
    "    # try:\n",
    "    #     cider_score = cider.compute(predictions=[\" \".join(p) for p in pred_texts], references=[\" \".join(l[0]) for l in label_texts])[\"CIDEr\"]\n",
    "    # except:\n",
    "    #     cider_score = 0.0\n",
    "\n",
    "    exact_matches = sum([\" \".join(p) == \" \".join(l[0]) for p, l in zip(pred_texts, label_texts)])\n",
    "    exact_match_score = exact_matches / len(pred_texts)\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": bleu_score,\n",
    "        \"ROUGE_L\": rouge_score,\n",
    "        # \"CIDEr\": cider_score,\n",
    "        \"Exact_Match\": exact_match_score,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76183611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LoRA Configuration ---\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = (len(dataset) // (TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NO_OF_EPOCHS\n",
    "warmup_steps = int(0.05 * total_steps)\n",
    "\n",
    "# --- SFT Trainer ---\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    eval_strategy=\"no\",\n",
    "    optim=\"adamw_torch_4bit\",\n",
    "    save_strategy=\"epoch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    output_dir=output_dir,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NO_OF_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps = 500,\n",
    "    # logging_nan_inf_filter=False,\n",
    "    # max_grad_norm=5.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d340723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiailize Wandb\n",
    "wandb.init(\n",
    "    project=PROJECT,\n",
    "    name=RUN_NAME,\n",
    "    config={\n",
    "        **training_args.to_dict(),\n",
    "        \"custom_config\": {\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"dataset\": DATASET,\n",
    "            \"task\": TASK,\n",
    "            \"LoRA_r\": LORA_R,\n",
    "            \"LoRA_alpha\": LORA_ALPHA,\n",
    "            \"LoRA_dropout\": LORA_DROPOUT,\n",
    "            \"target_modules\": TARGET_MODULES,\n",
    "        }\n",
    "    },\n",
    "    tags=TAGS,\n",
    "    group=GROUP,\n",
    "    notes=NOTES,\n",
    "    mode=\"online\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(model, log=\"all\", log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee25bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWandbCallback(WandbCallback):\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        if model is not None and state.global_step % 100 == 0:  # every 100 steps\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    wandb.log({f\"weights/{name}\": wandb.Histogram(param.data.cpu())}, step=state.global_step)\n",
    "\n",
    "        if hasattr(kwargs.get(\"optimizer\"), \"param_groups\") and state.global_step % 100 == 0:\n",
    "            lr = kwargs[\"optimizer\"].param_groups[0][\"lr\"]\n",
    "            # print(lr)\n",
    "            wandb.log({\"manual_learning_rate\": lr}, step=state.global_step)\n",
    "\n",
    "        # if model is not None and state.global_step % 100 == 0:  # every 100 steps\n",
    "        #     global tokenizer\n",
    "        #     bleu_metric=bleu_metric,\n",
    "        #     rouge_metric=rouge_metric,\n",
    "        \n",
    "        # return super().on_step_end(args, state, control, model=model, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    processing_class =tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CustomWandbCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Start Fine-tuning ---\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Final Model ---\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "print(f\"✅ Training complete. Model saved at {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dc1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('llava_lora_instagram', 'zip', 'llava_lora_instagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d1bb6",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de755b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference function\n",
    "def generate_caption(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Prepare the input prompt (same as training)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a social media influencer. Write a captivating Instagram caption for this image that will engage more viewers and boost interaction. Analyze the image to decide the tone of the caption.\"},\n",
    "            {\"type\": \"image\"}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    input_tensors = processor(text=inputs, images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        # output = model.generate(**input_tensors, max_new_tokens=80)\n",
    "        output = model.generate(\n",
    "            **input_tensors,\n",
    "            max_new_tokens=80,\n",
    "            repetition_penalty=1.2,   # Encourage less repetition\n",
    "            temperature=0.7,          # Add some randomness\n",
    "            top_p=0.9,                # Top-p sampling (nucleus sampling)\n",
    "            do_sample=True            # Enable sampling instead of greedy decoding\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = processor.batch_decode(output[:, input_tensors[\"input_ids\"].shape[1]:], skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b151bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./temp.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test1.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b06e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test3.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1923ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test4.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test5.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7588d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test6.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test7.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb96a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c563f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d142ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chat_with_model(messages, image=None):\n",
    "#     \"\"\"\n",
    "#     Function to send messages to the model and get a reply.\n",
    "#     - `messages`: current conversation list\n",
    "#     - `image`: PIL.Image if needed for the first user input\n",
    "#     \"\"\"\n",
    "#     # Prepare input\n",
    "#     if image:\n",
    "#         inputs = processor.apply_chat_template(messages, images=[image], return_tensors=\"pt\", tokenize=True, add_generation_prompt=True)\n",
    "#     else:\n",
    "#         inputs = processor.apply_chat_template(messages, return_tensors=\"pt\", tokenize=True, add_generation_prompt=True)\n",
    "    \n",
    "#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "#     # Generate\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=100,\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.9,\n",
    "#             repetition_penalty=1.1,\n",
    "#             do_sample=True\n",
    "#         )\n",
    "    \n",
    "#     # Decode output\n",
    "#     reply = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "#     return reply\n",
    "\n",
    "def chat_with_model(messages, image=None):\n",
    "    \"\"\"\n",
    "    Function to send messages to the model and get a reply.\n",
    "    \"\"\"\n",
    "    # Step 1: Create chat template\n",
    "    prompt_text = processor.tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: Encode inputs\n",
    "    if image:\n",
    "        inputs = processor(text=prompt_text, images=[image], return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        inputs = processor(text=prompt_text, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Step 3: Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # Step 4: Decode output\n",
    "    reply = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------\n",
    "# Start a conversation\n",
    "# ------------------\n",
    "\n",
    "# Step 1: Initial messages with an image\n",
    "image_path = \"test2.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a social media influencer. Write a catchy Instagram caption for this image.\"},\n",
    "            {\"type\": \"image\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# First model reply\n",
    "caption = chat_with_model(messages, image=image)\n",
    "print(f\"\\n🧠 Model: {caption}\")\n",
    "\n",
    "# Add model's response to messages\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\"type\": \"text\", \"text\": caption}]\n",
    "})\n",
    "\n",
    "# Step 2: Loop for continuous chat\n",
    "while True:\n",
    "    user_input = input(\"\\n💬 Your input (type 'quit' to stop): \")\n",
    "    \n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"👋 Ending chat. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    # Add user's new message\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": user_input}]\n",
    "    })\n",
    "    \n",
    "    # Get model's reply\n",
    "    model_reply = chat_with_model(messages)\n",
    "    print(f\"\\n🧠 Model: {model_reply}\")\n",
    "    \n",
    "    # Add model's reply back to messages\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": model_reply}]\n",
    "    })\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
