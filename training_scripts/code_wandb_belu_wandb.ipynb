{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03674a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fine-Tuning LLaVA 1.5 7B (HF version) for Instagram Captioning on Custom JSONL Data\n",
    "# # Compatible with RTX 3060 / T4 (12-16 GB GPUs)\n",
    "\n",
    "# !pip install -U \"transformers>=4.39.0\"\n",
    "# !pip install peft bitsandbytes\n",
    "# !pip install -U \"trl>=0.8.3\"\n",
    "# !pip install wandb\n",
    "# !pip install evaluate\n",
    "# !pip install datasets\n",
    "# !pip install rouge_score\n",
    "# !pip install torchao\n",
    "# # !pip install git+https://github.com/Maluuba/nlg-eval.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0c8ab",
   "metadata": {},
   "source": [
    "Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d77aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import torchao\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from torch.optim import AdamW\n",
    "# from datasets import load_metric\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from evaluate import load as load_metric\n",
    "from datasets import Dataset as HFDataset\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05374b",
   "metadata": {},
   "source": [
    "Setting Up WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc8c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"d707117fb8a8f4cf9916b4cf42fe630e09c93b6b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b11af11",
   "metadata": {},
   "source": [
    "Setting Up Script Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4db7cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# data configs\n",
    "DATASET = \"InstaCities1M\"\n",
    "BASE_IMAGES_DIR = \"/mnt/InstaCities1M/image/img_resized_1M/cities_instagram/\"\n",
    "BASE_CAPTIONS_DIR = \"/mnt/InstaCities1M/captions/captions_resized_1M/cities_instagram/\"\n",
    "OUTPUT_JSONL_PATH = './datasset_v1.jsonl'\n",
    "CITIES = ['newyork']\n",
    "\n",
    "LLAVA_CHAT_TEMPLATE = (\n",
    "    \"You are a social media influencer. Write a captivating Instagram caption for this image \"\n",
    "    \"that will engage more viewers and boost interaction. Analyze the image to decide the tone of the caption.\"\n",
    ")\n",
    "\n",
    "print(os.path.exists(BASE_IMAGES_DIR))\n",
    "print(os.path.exists(BASE_CAPTIONS_DIR))\n",
    "\n",
    "# Expirementation Details\n",
    "PROJECT = \"Snap2Caption\"\n",
    "RUN_NAME = \"llava-7b-ft-instagram-v2\"\n",
    "\n",
    "# model configs\n",
    "MODEL_NAME = \"LLaVA-7B-HF (LLaVA-1.5-7B)\"\n",
    "TASK = \"Image Captioning\"\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
    "\n",
    "# Search/Filter Tags\n",
    "TAGS = [\"llava\", \"image-captioning\", \"LoRA\", \"fine-tuning\"]\n",
    "GROUP = \"llava-instagram-experiments\"\n",
    "NOTES = \"Baseline fine-tuning on InstaCities1M with 4-bit quantized model and LoRA adapters.\"\n",
    "\n",
    "# Transformer Setting\n",
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "MODEL_SAVE_PATH = \"./llava_lora_instagram\"\n",
    "\n",
    "# Optimization Strategy\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "NO_OF_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "LOGGING_STEPS = 5\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4162bb",
   "metadata": {},
   "source": [
    "Verifying GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d74ba17-2810-4f8f-aa6c-538c4252144e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb426d",
   "metadata": {},
   "source": [
    "Dataset Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d41b55e3-a128-4550-9d21-e99b7b9ca052",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_files = []\n",
    "captions_files = []\n",
    "\n",
    "for city in CITIES:\n",
    "    img = BASE_IMAGES_DIR + city + '/' + np.array(os.listdir(BASE_IMAGES_DIR + city))\n",
    "    caption = BASE_CAPTIONS_DIR + city + '/' + np.array(os.listdir(BASE_CAPTIONS_DIR + city))\n",
    "    images_files.extend(img)\n",
    "    captions_files.extend(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4827eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean filenames\n",
    "image_ids = {os.path.splitext(os.path.basename(img))[0] for img in images_files}\n",
    "caption_ids = {os.path.splitext(os.path.basename(cap))[0] for cap in captions_files}\n",
    "\n",
    "# Now match\n",
    "common_ids = image_ids & caption_ids\n",
    "\n",
    "# Filter\n",
    "filtered_image_files = [img for img in images_files if os.path.splitext(os.path.basename(img))[0] in common_ids]\n",
    "filtered_caption_files = [cap for cap in captions_files if os.path.splitext(os.path.basename(cap))[0] in common_ids]\n",
    "\n",
    "images_files = filtered_image_files\n",
    "captions_files = filtered_caption_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5820b8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_files), len(captions_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85061bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:33<00:00, 3024.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Worker function ---\n",
    "def process_pair(i):\n",
    "    try:\n",
    "        img_path = images_files[i]\n",
    "        caption_path = captions_files[i]\n",
    "\n",
    "        with open(caption_path, 'r', encoding='utf-8') as f:\n",
    "            caption = f.read().strip().replace('\\n', ' ')\n",
    "            if not caption:\n",
    "                return None\n",
    "\n",
    "        # Create messages field directly\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": LLAVA_CHAT_TEMPLATE},\n",
    "                    {\"type\": \"image\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": caption}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"image_path\": img_path,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Multiprocessing ---\n",
    "with Pool(cpu_count()) as pool:\n",
    "    results = list(tqdm(pool.imap(process_pair, range(len(images_files))), total=len(images_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2097a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL created: ./datasset_v1.jsonl with 100000 samples.\n"
     ]
    }
   ],
   "source": [
    "data = [entry for entry in results if entry is not None]\n",
    "\n",
    "# --- Write JSONL File ---\n",
    "with open(OUTPUT_JSONL_PATH, 'w', encoding='utf-8') as f:\n",
    "    for entry in data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"JSONL created: {OUTPUT_JSONL_PATH} with {len(data)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0fbf6",
   "metadata": {},
   "source": [
    "Configuring Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b207172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "model_id = MODEL_ID\n",
    "data_path = OUTPUT_JSONL_PATH  # path to your formatted JSONL file\n",
    "output_dir = MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6363d528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d59a05e4e74d1594734540294f1a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# --- Model Loading (4bit Quantization) ---\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "\n",
    ")\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,\n",
    "#     llm_int8_threshold=6.0,\n",
    "#     llm_int8_has_fp16_weight=False\n",
    "# )\n",
    "\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "tokenizer.chat_template = (\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if message['role'] == 'user' %}USER: {% else %}ASSISTANT: {% endif %}\"\n",
    "    \"{% for item in message['content'] %}\"\n",
    "    \"{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}{{ eos_token }}{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69cd12c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56616d8e21534f6c80442975d882998b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_image(example):\n",
    "    example[\"image\"] = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "    return example\n",
    "\n",
    "# Load and preprocess dataset with pre-formatted messages\n",
    "dataset = load_dataset(\"json\", data_files=data_path)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef0f954e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_path': '/mnt/InstaCities1M/image/img_resized_1M/cities_instagram/newyork/1480879485913200243.jpg',\n",
       " 'messages': [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'You are a social media influencer. Write a captivating Instagram caption for this image that will engage more viewers and boost interaction. Analyze the image to decide the tone of the caption.'},\n",
       "    {'type': 'image', 'text': None}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Where the dreams come true. #newyork #newyorkcity #rooseveltisland #dreams #manhattan #usa #bowtie #pocketsquare #handmade #newjersey #style #stylish #styleblogger #fashion #fashionstyle #fashionblogger #gay #gaymen #gayboy #gayguy #men #mensuit #menstyle #menswear #nyc #suitandtie #suit #shirt #ootd #onlinestore'}]}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dcef392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 samples.\n",
      "99900\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Read JSONL manually\n",
    "dataset = []\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line.strip())\n",
    "        dataset.append(example)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples.\")\n",
    "\n",
    "dataset = HFDataset.from_list(dataset)\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.001, seed=42)\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "495cbfae-4cf0-465b-bc79-b009caee0d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Dataset.map of Dataset({\n",
       "    features: ['image_path', 'messages'],\n",
       "    num_rows: 99900\n",
       "})>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa9d93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SimpleDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "# # Wrap\n",
    "# train_dataset = SimpleDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba85173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def build_compute_metrics(tokenizer):\n",
    "    bleu = load(\"bleu\")\n",
    "    rouge = load(\"rouge\")\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "\n",
    "        print(eval_preds)\n",
    "        \n",
    "        preds, labels = eval_preds\n",
    "\n",
    "        # Convert to numpy arrays if not already\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        preds = np.asarray(preds)\n",
    "        labels = np.asarray(labels)\n",
    "\n",
    "        # Special trick for generative models: remove -100\n",
    "        labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "        # Decode\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # BLEU expects a list of list of references\n",
    "        bleu_score = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])['bleu']\n",
    "\n",
    "        rouge_score = rouge.compute(predictions=decoded_preds, references=decoded_labels)['rougeL']\n",
    "\n",
    "        return {\n",
    "            \"bleu\": round(bleu_score * 100, 2),\n",
    "            \"rougeL\": round(rouge_score * 100, 2),\n",
    "        }\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "# def build_compute_metrics(tokenizer):\n",
    "#     bleu = load_metric(\"bleu\")\n",
    "#     rouge = load_metric(\"rouge\")\n",
    "\n",
    "#     def compute_metrics(eval_preds):\n",
    "#         predictions, labels = eval_preds\n",
    "\n",
    "#         # Decode tokenized outputs into text\n",
    "#         preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#         refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#         # Compute BLEU\n",
    "#         bleu_score = bleu.compute(predictions=preds, references=[[r] for r in refs])['bleu']\n",
    "\n",
    "#         # Compute ROUGE\n",
    "#         rouge_score = rouge.compute(predictions=preds, references=refs)['rougeL']\n",
    "\n",
    "#         return {\n",
    "#             \"bleu\": bleu_score,\n",
    "#             \"rougeL\": rouge_score,\n",
    "#         }\n",
    "\n",
    "#     return compute_metrics\n",
    "\n",
    "# # def compute_metrics(eval_preds, tokenizer):\n",
    "# #     preds, labels = eval_preds\n",
    "# #     if hasattr(preds, \"predictions\"):\n",
    "# #         preds = preds.predictions\n",
    "# #     preds = preds.argmax(dim=-1)\n",
    "# #     labels = labels.long()\n",
    "\n",
    "# #     preds = preds.tolist()\n",
    "# #     labels = labels.tolist()\n",
    "\n",
    "# #     pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "# #     label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "# #     pred_texts = [text.strip().split() for text in pred_texts]\n",
    "# #     label_texts = [[text.strip().split()] for text in label_texts]\n",
    "\n",
    "# #     try:\n",
    "# #         bleu_score = bleu.compute(predictions=pred_texts, references=label_texts)[\"bleu\"]\n",
    "# #     except Exception as e:\n",
    "# #         print(f\"Error computing BLEU: {e}\")\n",
    "# #         bleu_score = 0.0\n",
    "# #     try:\n",
    "# #         rouge_score = rouge.compute(predictions=[\" \".join(p) for p in pred_texts], references=[\" \".join(l[0]) for l in label_texts])[\"rougeL\"]\n",
    "# #     except Exception as e:\n",
    "# #         print(f\"Error computing ROUGE: {e}\")\n",
    "# #         rouge_score = 0.0\n",
    "\n",
    "# #     exact_matches = sum([\" \".join(p) == \" \".join(l[0]) for p, l in zip(pred_texts, label_texts)])\n",
    "# #     exact_match_score = exact_matches / len(pred_texts)\n",
    "\n",
    "# #     return {\n",
    "# #         \"BLEU\": bleu_score,\n",
    "# #         \"ROUGE_L\": rouge_score,\n",
    "# #         \"Exact_Match\": exact_match_score,\n",
    "# #     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76183611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LoRA Configuration ---\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81c4fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = (len(dataset) // (TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NO_OF_EPOCHS\n",
    "warmup_steps = int(0.05 * total_steps)\n",
    "\n",
    "# --- SFT Trainer ---\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    dataloader_num_workers=4,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    \n",
    "    optim=\"adamw_torch_4bit\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps = warmup_steps,\n",
    "    num_train_epochs=NO_OF_EPOCHS,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    \n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_accumulation_steps=8,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=LOGGING_STEPS,\n",
    "    \n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    \n",
    "    save_strategy=\"epoch\",\n",
    "    output_dir=output_dir,\n",
    "\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d340723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>startup/loss</td><td>▁</td></tr><tr><td>startup/step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr><tr><td>train/grad_norm</td><td>▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>startup/loss</td><td>0</td></tr><tr><td>startup/step</td><td>0</td></tr><tr><td>train/epoch</td><td>0.0016</td></tr><tr><td>train/global_step</td><td>5</td></tr><tr><td>train/grad_norm</td><td>65.30002</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>48.9635</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llava-7b-ft-instagram-v2</strong> at: <a href='https://wandb.ai/jd6065-new-york-university/Snap2Caption/runs/u7yvb3fq' target=\"_blank\">https://wandb.ai/jd6065-new-york-university/Snap2Caption/runs/u7yvb3fq</a><br> View project at: <a href='https://wandb.ai/jd6065-new-york-university/Snap2Caption' target=\"_blank\">https://wandb.ai/jd6065-new-york-university/Snap2Caption</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250427_102155-u7yvb3fq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20250427_102827-j3jxszor</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jd6065-new-york-university/Snap2Caption/runs/j3jxszor' target=\"_blank\">llava-7b-ft-instagram-v2</a></strong> to <a href='https://wandb.ai/jd6065-new-york-university/Snap2Caption' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jd6065-new-york-university/Snap2Caption' target=\"_blank\">https://wandb.ai/jd6065-new-york-university/Snap2Caption</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jd6065-new-york-university/Snap2Caption/runs/j3jxszor' target=\"_blank\">https://wandb.ai/jd6065-new-york-university/Snap2Caption/runs/j3jxszor</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jd6065-new-york-university/Snap2Caption/runs/j3jxszor?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x71a558caf3e0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intiailize Wandb\n",
    "wandb.init(\n",
    "    project=PROJECT,\n",
    "    name=RUN_NAME,\n",
    "    config={\n",
    "        **training_args.to_dict(),\n",
    "        \"custom_config\": {\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"dataset\": DATASET,\n",
    "            \"task\": TASK,\n",
    "            \"LoRA_r\": LORA_R,\n",
    "            \"LoRA_alpha\": LORA_ALPHA,\n",
    "            \"LoRA_dropout\": LORA_DROPOUT,\n",
    "            \"target_modules\": TARGET_MODULES,\n",
    "        }\n",
    "    },\n",
    "    tags=TAGS,\n",
    "    group=GROUP,\n",
    "    notes=NOTES,\n",
    "    mode=\"online\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbff8cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"startup/step\": 0, \"startup/loss\": 0.0})\n",
    "\n",
    "wandb.watch(model, log=\"all\", log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "435b257c-73bf-46ac-900e-1c44967f3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "def build_compute_metrics(tokenizer):\n",
    "    bleu = load(\"bleu\")\n",
    "    rouge = load(\"rouge\")\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds = eval_preds.predictions\n",
    "        labels = eval_preds.label_ids\n",
    "\n",
    "        # If preds are tuple (logits, ), take first\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Take argmax if needed\n",
    "        if preds.ndim == 3:\n",
    "            preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "        # Clean labels (-100 to pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "        # Convert to list\n",
    "        preds = preds.tolist()\n",
    "        labels = labels.tolist()\n",
    "\n",
    "        # Decode\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        # Clean whitespace\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        # Compute BLEU and ROUGE\n",
    "        bleu_score = bleu.compute(predictions=decoded_preds, references=[[r] for r in decoded_labels])[\"bleu\"]\n",
    "        rouge_score = rouge.compute(predictions=decoded_preds, references=decoded_labels)[\"rougeL\"]\n",
    "\n",
    "        return {\n",
    "            \"bleu\": round(bleu_score * 100, 2),\n",
    "            \"rougeL\": round(rouge_score * 100, 2),\n",
    "        }\n",
    "\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6860b729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3310d6ae9fd74d779fcb99376c45bbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/99900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6da9f36631462591c263834eb91bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/99900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b6011af87a4752b9d1809837bda683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/99900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94ef178514b420c8854125e1e15ca42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/99900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a62e2908584a4aadbcccfc073cd0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1905234fac44dd6bfca522cd70e15ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bae4fcd69e4ee68a758a6f4384797a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931bb19dbaae4579a6d2fd10bc4b3a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# # # compute_metrics_func = compute_metrics(tokenizer)\n",
    "# # compute_metrics_func = build_compute_metrics(tokenizer)\n",
    "\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset ,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     peft_config=lora_config,\n",
    "#     processing_class =tokenizer,\n",
    "#     # compute_metrics=compute_metrics_func,\n",
    "#     # callbacks=[DynamicEvalSubsetCallback(eval_dataset, subset_size=100)],\n",
    "# )\n",
    "compute_metrics_func = build_compute_metrics(tokenizer)\n",
    "\n",
    "import random\n",
    "from trl import SFTTrainer\n",
    "\n",
    "class SFTTrainerEvalSampling(SFTTrainer):\n",
    "    def __init__(self, *args, eval_sample_size=16, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eval_sample_size = eval_sample_size\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        '''\n",
    "        Samples the evaluation dataset and returns a subset \n",
    "        of size self.eval_sample_size.\n",
    "        '''\n",
    "        if eval_dataset is None:\n",
    "            eval_dataset = self.eval_dataset\n",
    "\n",
    "        idxs = random.sample(range(len(eval_dataset)), self.eval_sample_size)\n",
    "        print(idxs)\n",
    "        eval_subset = eval_dataset.select(idxs)\n",
    "\n",
    "        return super().get_eval_dataloader(eval_subset)\n",
    "\n",
    "trainer = SFTTrainerEvalSampling(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,    # pass full 10k dataset here\n",
    "    eval_sample_size=100,               # choose sample size you want\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    # compute_metrics_func = build_compute_metrics(tokenizer)\n",
    "\n",
    "    compute_metrics=compute_metrics_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cdec29ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='31210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   17/31210 01:36 < 55:30:28, 0.16 it/s, Epoch 0.01/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>48.963500</td>\n",
       "      <td>6.227513</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>19.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>50.187700</td>\n",
       "      <td>6.210383</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>19.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>49.311300</td>\n",
       "      <td>6.197471</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>19.410000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81, 14, 3, 94, 35, 31, 28, 17, 13, 86, 69, 11, 75, 54, 4, 97, 88, 27, 29, 64, 77, 84, 71, 25, 89, 53, 93, 57, 95, 0, 20, 90, 43, 79, 19, 82, 67, 6, 5, 24, 62, 22, 68, 58, 38, 16, 51, 2, 46, 99, 34, 7, 60, 61, 66, 18, 40, 39, 23, 36, 12, 85, 52, 98, 44, 74, 63, 59, 47, 8, 33, 87, 26, 83, 49, 80, 96, 32, 21, 30, 37, 76, 92, 48, 45, 72, 56, 55, 10, 73, 78, 15, 70, 1, 50, 65, 9, 91, 41, 42]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 91, 40, 27, 83, 63, 50, 82, 58, 18, 33, 17, 31, 71, 68, 89, 74, 54, 95, 51, 46, 28, 88, 65, 94, 11, 6, 14, 19, 20, 92, 8, 49, 48, 59, 32, 1, 43, 79, 7, 62, 56, 34, 66, 77, 67, 41, 21, 60, 90, 96, 10, 29, 0, 16, 64, 81, 44, 73, 97, 86, 42, 12, 9, 23, 98, 84, 61, 70, 15, 38, 3, 76, 26, 25, 36, 80, 75, 35, 93, 2, 39, 30, 87, 4, 45, 57, 52, 85, 99, 55, 69, 53, 22, 24, 37, 13, 5, 78, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 57, 15, 31, 28, 8, 43, 2, 75, 70, 29, 91, 95, 0, 9, 80, 7, 89, 94, 4, 42, 85, 65, 30, 35, 62, 27, 69, 16, 60, 96, 90, 52, 24, 12, 77, 55, 22, 73, 26, 82, 63, 46, 3, 93, 41, 54, 6, 56, 25, 98, 21, 67, 97, 64, 45, 34, 87, 81, 61, 11, 17, 59, 49, 84, 79, 47, 51, 86, 92, 37, 99, 40, 83, 5, 13, 36, 23, 33, 44, 1, 50, 20, 72, 38, 88, 74, 58, 68, 14, 48, 53, 39, 19, 78, 76, 18, 71, 10, 32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Start Fine-tuning ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/transformers/trainer.py:3782\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3780\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3782\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3784\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/accelerate/accelerator.py:2450\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2449\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2450\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2451\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2452\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Start Fine-tuning ---\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1be950-86b3-447c-928c-8941503df8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a1cccc-f316-44ae-a525-35ba401db138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19051a-b6c4-4c92-ba3d-bd24c0914955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3292c-235b-4743-b497-35f3e78ad700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Final Model ---\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "print(f\"✅ Training complete. Model saved at {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dc1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('llava_lora_instagram', 'zip', 'llava_lora_instagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d1bb6",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de755b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference function\n",
    "def generate_caption(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Prepare the input prompt (same as training)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a social media influencer. Write a captivating Instagram caption for this image that will engage more viewers and boost interaction. Analyze the image to decide the tone of the caption.\"},\n",
    "            {\"type\": \"image\"}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    input_tensors = processor(text=inputs, images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        # output = model.generate(**input_tensors, max_new_tokens=80)\n",
    "        output = model.generate(\n",
    "            **input_tensors,\n",
    "            max_new_tokens=80,\n",
    "            repetition_penalty=1.2,   # Encourage less repetition\n",
    "            temperature=0.7,          # Add some randomness\n",
    "            top_p=0.9,                # Top-p sampling (nucleus sampling)\n",
    "            do_sample=True            # Enable sampling instead of greedy decoding\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = processor.batch_decode(output[:, input_tensors[\"input_ids\"].shape[1]:], skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b151bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./temp.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test1.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b06e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test3.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1923ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test4.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test5.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7588d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test6.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test7.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb96a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c563f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d142ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chat_with_model(messages, image=None):\n",
    "#     \"\"\"\n",
    "#     Function to send messages to the model and get a reply.\n",
    "#     - `messages`: current conversation list\n",
    "#     - `image`: PIL.Image if needed for the first user input\n",
    "#     \"\"\"\n",
    "#     # Prepare input\n",
    "#     if image:\n",
    "#         inputs = processor.apply_chat_template(messages, images=[image], return_tensors=\"pt\", tokenize=True, add_generation_prompt=True)\n",
    "#     else:\n",
    "#         inputs = processor.apply_chat_template(messages, return_tensors=\"pt\", tokenize=True, add_generation_prompt=True)\n",
    "    \n",
    "#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "#     # Generate\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=100,\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.9,\n",
    "#             repetition_penalty=1.1,\n",
    "#             do_sample=True\n",
    "#         )\n",
    "    \n",
    "#     # Decode output\n",
    "#     reply = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "#     return reply\n",
    "\n",
    "def chat_with_model(messages, image=None):\n",
    "    \"\"\"\n",
    "    Function to send messages to the model and get a reply.\n",
    "    \"\"\"\n",
    "    # Step 1: Create chat template\n",
    "    prompt_text = processor.tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: Encode inputs\n",
    "    if image:\n",
    "        inputs = processor(text=prompt_text, images=[image], return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        inputs = processor(text=prompt_text, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Step 3: Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # Step 4: Decode output\n",
    "    reply = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------\n",
    "# Start a conversation\n",
    "# ------------------\n",
    "\n",
    "# Step 1: Initial messages with an image\n",
    "image_path = \"test2.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a social media influencer. Write a catchy Instagram caption for this image.\"},\n",
    "            {\"type\": \"image\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# First model reply\n",
    "caption = chat_with_model(messages, image=image)\n",
    "print(f\"\\n🧠 Model: {caption}\")\n",
    "\n",
    "# Add model's response to messages\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\"type\": \"text\", \"text\": caption}]\n",
    "})\n",
    "\n",
    "# Step 2: Loop for continuous chat\n",
    "while True:\n",
    "    user_input = input(\"\\n💬 Your input (type 'quit' to stop): \")\n",
    "    \n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"👋 Ending chat. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    # Add user's new message\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": user_input}]\n",
    "    })\n",
    "    \n",
    "    # Get model's reply\n",
    "    model_reply = chat_with_model(messages)\n",
    "    print(f\"\\n🧠 Model: {model_reply}\")\n",
    "    \n",
    "    # Add model's reply back to messages\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": model_reply}]\n",
    "    })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
