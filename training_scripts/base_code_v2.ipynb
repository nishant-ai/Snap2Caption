{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning LLaVA 1.5 7B (HF version) for Instagram Captioning on Custom JSONL Data\n",
    "# Compatible with RTX 3060 / T4 (12-16 GB GPUs)\n",
    "\n",
    "!pip install -U \"transformers>=4.39.0\"\n",
    "!pip install peft bitsandbytes\n",
    "!pip install -U \"trl>=0.8.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a89f75",
   "metadata": {},
   "source": [
    "Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d999c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import Dataset\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_metric\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from transformers.integrations import WandbCallback\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83618db1",
   "metadata": {},
   "source": [
    "Setting Up WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"d707117fb8a8f4cf9916b4cf42fe630e09c93b6b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38569c91",
   "metadata": {},
   "source": [
    "Setting Up Script Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data configs\n",
    "DATASET = \"InstaCities1M\"\n",
    "BASE_IMAGES_DIR = \"/mnt/InstaCities1M/img_resized_1M/cities_instagram/\"\n",
    "BASE_CAPTIONS_DIR = \"/mnt/InstaCities1M/captions_resized_1M/cities_instagram/\"\n",
    "OUTPUT_JSONL_PATH = './datasset_v1.jsonl'\n",
    "CITIES = ['newyork']\n",
    "\n",
    "LLAVA_CHAT_TEMPLATE = (\n",
    "    \"You are a social media influencer. Write a captivating Instagram caption for this image \"\n",
    "    \"that will engage more viewers and boost interaction. Analyze the image to decide the tone of the caption.\"\n",
    ")\n",
    "\n",
    "print(os.path.exists(BASE_IMAGES_DIR))\n",
    "print(os.path.exists(BASE_CAPTIONS_DIR))\n",
    "\n",
    "# Expirementation Details\n",
    "PROJECT = \"Snap2Caption\"\n",
    "RUN_NAME = \"llava-7b-ft-instagram-v1\"\n",
    "\n",
    "# model configs\n",
    "MODEL_NAME = \"LLaVA-7B-HF (LLaVA-1.5-7B)\"\n",
    "TASK = \"Image Captioning\"\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "# Search/Filter Tags\n",
    "TAGS = [\"llava\", \"image-captioning\", \"LoRA\", \"fine-tuning\"]\n",
    "GROUP = \"llava-instagram-experiments\"\n",
    "NOTES = \"Baseline fine-tuning on InstaCities1M with 4-bit quantized model and LoRA adapters.\"\n",
    "\n",
    "# Transformer Setting\n",
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "MODEL_SAVE_PATH = \"./llava_lora_instagram\"\n",
    "\n",
    "# Optimization Strategy\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "NO_OF_EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "LOGGING_STEPS = 100\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8338b7",
   "metadata": {},
   "source": [
    "Verifying GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01df042",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103de6f",
   "metadata": {},
   "source": [
    "Dataset Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5015894",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_files = []\n",
    "captions_files = []\n",
    "\n",
    "for city in CITIES:\n",
    "    img = BASE_IMAGES_DIR + city + '/' + np.array(os.listdir(BASE_IMAGES_DIR + city))\n",
    "    caption = BASE_CAPTIONS_DIR + city + '/' + np.array(os.listdir(BASE_CAPTIONS_DIR + city))\n",
    "    images_files.extend(img)\n",
    "    captions_files.extend(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5557a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean filenames\n",
    "image_ids = {os.path.splitext(os.path.basename(img))[0] for img in images_files}\n",
    "caption_ids = {os.path.splitext(os.path.basename(cap))[0] for cap in captions_files}\n",
    "\n",
    "# Now match\n",
    "common_ids = image_ids & caption_ids\n",
    "\n",
    "# Filter\n",
    "filtered_image_files = [img for img in images_files if os.path.splitext(os.path.basename(img))[0] in common_ids]\n",
    "filtered_caption_files = [cap for cap in captions_files if os.path.splitext(os.path.basename(cap))[0] in common_ids]\n",
    "\n",
    "images_files = filtered_image_files\n",
    "captions_files = filtered_caption_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3bd892",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images_files), len(captions_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Worker function ---\n",
    "def process_pair(i):\n",
    "    try:\n",
    "        img_path = images_files[i]\n",
    "        caption_path = captions_files[i]\n",
    "\n",
    "        with open(caption_path, 'r', encoding='utf-8') as f:\n",
    "            caption = f.read().strip().replace('\\n', ' ')\n",
    "            if not caption:\n",
    "                return None\n",
    "\n",
    "        # Create messages field directly\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": LLAVA_CHAT_TEMPLATE},\n",
    "                    {\"type\": \"image\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": caption}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"image_path\": img_path,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Multiprocessing ---\n",
    "with Pool(cpu_count()) as pool:\n",
    "    results = list(tqdm(pool.imap(process_pair, range(len(images_files))), total=len(images_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [entry for entry in results if entry is not None]\n",
    "\n",
    "# --- Write JSONL File ---\n",
    "with open(OUTPUT_JSONL_PATH, 'w', encoding='utf-8') as f:\n",
    "    for entry in data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"JSONL created: {OUTPUT_JSONL_PATH} with {len(data)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27ac9d4",
   "metadata": {},
   "source": [
    "Configuring Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fb514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "model_id = MODEL_ID\n",
    "data_path = OUTPUT_JSONL_PATH  # path to your formatted JSONL file\n",
    "output_dir = MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7822cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Loading (4bit Quantization) ---\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "tokenizer.chat_template = (\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if message['role'] == 'user' %}USER: {% else %}ASSISTANT: {% endif %}\"\n",
    "    \"{% for item in message['content'] %}\"\n",
    "    \"{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if message['role'] == 'assistant' %}{{ eos_token }}{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2287eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(example):\n",
    "    example[\"image\"] = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "    return example\n",
    "\n",
    "# Load and preprocess dataset with pre-formatted messages\n",
    "dataset = load_dataset(\"json\", data_files=data_path)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de86fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa1b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSONL manually\n",
    "dataset = []\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line.strip())\n",
    "        dataset.append(example)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples.\")\n",
    "\n",
    "dataset = Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Wrap\n",
    "train_dataset = SimpleDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load_metric(\"bleu\")\n",
    "rouge = load_metric(\"rouge\")\n",
    "cider = load_metric(\"cider\")  # cider is available in some versions or custom\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    preds = preds.argmax(dim=-1) if hasattr(preds, \"argmax\") else preds\n",
    "    preds = preds.tolist()\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    # Assuming you have a decode function:\n",
    "    pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Format inputs properly\n",
    "    pred_texts = [text.strip().split() for text in pred_texts]\n",
    "    label_texts = [[text.strip().split()] for text in label_texts]  # note double brackets for corpus\n",
    "\n",
    "    bleu_score = bleu.compute(predictions=pred_texts, references=label_texts)[\"bleu\"]\n",
    "    rouge_score = rouge.compute(predictions=[\" \".join(p) for p in pred_texts], references=[\" \".join(l[0]) for l in label_texts])[\"rougeL\"]\n",
    "    \n",
    "    # cider expects slightly different input\n",
    "    try:\n",
    "        cider_score = cider.compute(predictions=[\" \".join(p) for p in pred_texts], references=[\" \".join(l[0]) for l in label_texts])[\"CIDEr\"]\n",
    "    except:\n",
    "        cider_score = 0.0\n",
    "\n",
    "    # Exact Match (if captions are identical)\n",
    "    exact_matches = sum([\" \".join(p) == \" \".join(l[0]) for p, l in zip(pred_texts, label_texts)])\n",
    "    exact_match_score = exact_matches / len(pred_texts)\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": bleu_score,\n",
    "        \"ROUGE_L\": rouge_score,\n",
    "        \"CIDEr\": cider_score,\n",
    "        \"Exact_Match\": exact_match_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LoRA Configuration ---\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5088d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SFT Trainer ---\n",
    "training_args = TrainingArguments(\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    eval_strategy=\"no\",\n",
    "    optim=\"adamw_torch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    output_dir=output_dir,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NO_OF_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    ")\n",
    "\n",
    "# Intiailize Wandb\n",
    "wandb.init(\n",
    "    project=PROJECT,\n",
    "    name=RUN_NAME,\n",
    "    config={\n",
    "        **training_args.to_dict(),\n",
    "        \"custom_config\": {\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"dataset\": DATASET,\n",
    "            \"task\": TASK,\n",
    "            \"LoRA_r\": LORA_R,\n",
    "            \"LoRA_alpha\": LORA_ALPHA,\n",
    "            \"LoRA_dropout\": LORA_DROPOUT,\n",
    "            \"target_modules\": TARGET_MODULES,\n",
    "        }\n",
    "    },\n",
    "    tags=TAGS,\n",
    "    group=GROUP,\n",
    "    notes=NOTES,\n",
    "    mode=\"online\"\n",
    ")\n",
    "\n",
    "wandb.watch(model, log=\"all\", log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "total_steps = (len(dataset) // (TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NO_OF_EPOCHS\n",
    "warmup_steps = int(0.05 * total_steps)\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94aa38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWandbCallback(WandbCallback):\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        if model is not None and state.global_step % 100 == 0:  # every 100 steps\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    wandb.log({f\"weights/{name}\": wandb.Histogram(param.data.cpu())}, step=state.global_step)\n",
    "        return super().on_step_end(args, state, control, model=model, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e22e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    processing_class =tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[CustomWandbCallback()],\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b46a1",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Start Fine-tuning ---\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9fd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Final Model ---\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "print(f\"âœ… Training complete. Model saved at {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('llava_lora_instagram', 'zip', 'llava_lora_instagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c570b5",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference function\n",
    "def generate_caption(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Prepare the input prompt (same as training)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a social media influencer. Write a captivating Instagram caption for this image that will engage more viewers and boost interaction. Analyze the image to decide the tone of the caption.\"},\n",
    "            {\"type\": \"image\"}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    input_tensors = processor(text=inputs, images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        # output = model.generate(**input_tensors, max_new_tokens=80)\n",
    "        output = model.generate(\n",
    "            **input_tensors,\n",
    "            max_new_tokens=80,\n",
    "            repetition_penalty=1.2,   # Encourage less repetition\n",
    "            temperature=0.7,          # Add some randomness\n",
    "            top_p=0.9,                # Top-p sampling (nucleus sampling)\n",
    "            do_sample=True            # Enable sampling instead of greedy decoding\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = processor.batch_decode(output[:, input_tensors[\"input_ids\"].shape[1]:], skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./temp.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c4326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test1.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64805ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test3.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test4.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ee258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test5.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca20ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test6.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage\n",
    "caption = generate_caption(\"./test7.jpg\")\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986878e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b5fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chat_with_model(messages, image=None):\n",
    "#     \"\"\"\n",
    "#     Function to send messages to the model and get a reply.\n",
    "#     - `messages`: current conversation list\n",
    "#     - `image`: PIL.Image if needed for the first user input\n",
    "#     \"\"\"\n",
    "#     # Prepare input\n",
    "#     if image:\n",
    "#         inputs = processor.apply_chat_template(messages, images=[image], return_tensors=\"pt\", tokenize=True, add_generation_prompt=True)\n",
    "#     else:\n",
    "#         inputs = processor.apply_chat_template(messages, return_tensors=\"pt\", tokenize=True, add_generation_prompt=True)\n",
    "    \n",
    "#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "#     # Generate\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=100,\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.9,\n",
    "#             repetition_penalty=1.1,\n",
    "#             do_sample=True\n",
    "#         )\n",
    "    \n",
    "#     # Decode output\n",
    "#     reply = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "#     return reply\n",
    "\n",
    "def chat_with_model(messages, image=None):\n",
    "    \"\"\"\n",
    "    Function to send messages to the model and get a reply.\n",
    "    \"\"\"\n",
    "    # Step 1: Create chat template\n",
    "    prompt_text = processor.tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: Encode inputs\n",
    "    if image:\n",
    "        inputs = processor(text=prompt_text, images=[image], return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        inputs = processor(text=prompt_text, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Step 3: Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # Step 4: Decode output\n",
    "    reply = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9291f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------\n",
    "# Start a conversation\n",
    "# ------------------\n",
    "\n",
    "# Step 1: Initial messages with an image\n",
    "image_path = \"test2.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"You are a social media influencer. Write a catchy Instagram caption for this image.\"},\n",
    "            {\"type\": \"image\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# First model reply\n",
    "caption = chat_with_model(messages, image=image)\n",
    "print(f\"\\nðŸ§  Model: {caption}\")\n",
    "\n",
    "# Add model's response to messages\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\"type\": \"text\", \"text\": caption}]\n",
    "})\n",
    "\n",
    "# Step 2: Loop for continuous chat\n",
    "while True:\n",
    "    user_input = input(\"\\nðŸ’¬ Your input (type 'quit' to stop): \")\n",
    "    \n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"ðŸ‘‹ Ending chat. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    # Add user's new message\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": user_input}]\n",
    "    })\n",
    "    \n",
    "    # Get model's reply\n",
    "    model_reply = chat_with_model(messages)\n",
    "    print(f\"\\nðŸ§  Model: {model_reply}\")\n",
    "    \n",
    "    # Add model's reply back to messages\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": model_reply}]\n",
    "    })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
