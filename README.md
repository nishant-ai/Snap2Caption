# Snap2Caption

> *Turn any Instagram photo into a scrollâ€‘stopping post, complete with a tailored caption and trending hashtags, in under two seconds.*

---

## Meet **Aisha**, Our Customer  *(UnitÂ 1 â€“ Value Proposition)*

Aisha is a solo lifestyle creator who posts colourful city snapshots every evening after work. She loves sharing, hates wordâ€‘smithing, and checks insights obsessively. Snap2Caption exists for creators like her: upload a photo, get a punchy caption in her voice plus hashtags tuned to whatâ€™s hot **right now**.

**Design commitments influenced by Aisha**

| Promise                | Design consequence                                             |
| ---------------------- | -------------------------------------------------------------- |
| Zero friction          | Single REST endpoint (`/generate-caption`) callable via `curl` |
| Timely trend awareness | Overnight automated reâ€‘training on fresh InstaCities data      |
| Mobileâ€‘first speed     | P90 endâ€‘toâ€‘end latencyÂ <Â 2â€¯s; GPUâ€‘backed autoscaling           |

**Scale (MayÂ 2025)**

| Component             | Value today                      | 12â€‘month projection   |
| --------------------- | -------------------------------- | --------------------- |
| Training data volume  | 50â€¯GB (100â€¯k images)             | 1â€¯M images (â‰ˆâ€¯500â€¯GB) |
| Fineâ€‘tuned model size | 14â€¯GB FP16 (LLaVAâ€‘1.5â€‘7BÂ + LoRA) | 8â€¯GB INT8 distilled   |
| Training duration     | 5â€¯h on 4Ã—Â A100â€‘80G               | 3â€¯h with DDPÂ + FP8    |
| Peak inference load   | 300Â reqâ€¯/â€¯h                      | 5â€¯000Â reqâ€¯/â€¯h         |

---

## From Pixels to Parquet â€”â€¯**Data Foundations**  *(UnitÂ 8)*

The nightly ETL pipeline fetches raw JPEGs from InstaCities, validates and resizes them to `300Ã—300`, wraps each in a chat template, then materialises Parquet manifests. Splits are stratified 80â€¯/â€¯10â€¯/â€¯10 by *user* to prevent leakage. Processed images live in a Cephâ€‘backed object store; metadata in Parquet drives training and offline tests.

| Persistent mount               | Purpose                   | Size  |
| ------------------------------ | ------------------------- | ----- |
| `/mnt/object/snap2caption-raw` | RawÂ & processed images    | 50â€¯GB |
| `/mnt/block/checkpoints`       | Model & tokenizer weights | 25â€¯GB |
| `/mnt/block/experiments`       | Offline W\&B cache        | 10â€¯GB |

During production, each inference and user rating is streamed via Kafka into `/mnt/block/online_events`. A Spark batch joins engagement metrics, computes KLâ€‘drift, andâ€”if driftÂ >Â 0.15â€”raises a Prometheus alert that triggers the automated reâ€‘train job.

---

## Teaching the Model â€”â€¯**Training & Experimentation**  *(UnitsÂ 4Â &Â 5)*

We cast captionÂ + hashtag generation as *imageâ€‘conditioned sequence generation*. The model of record is **LLaVAâ€‘1.5â€‘7B** fineâ€‘tuned with rankâ€‘16 LoRA adapters.

| Element                  | Implementation reference                                                                                                                                                           |
| ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Training scripts         | [https://github.com/nishant-ai/Snap2Caption/tree/main/training\_scripts](https://github.com/nishant-ai/Snap2Caption/tree/main/training_scripts)                                    |
| Full training pipeline   | [https://github.com/nishant-ai/Snap2Caption/tree/training-pipeline](https://github.com/nishant-ai/Snap2Caption/tree/training-pipeline)                                             |
| W\&B experiment notebook | [https://github.com/nishant-ai/Snap2Caption/blob/main/training\_scripts/code\_wandb.ipynb](https://github.com/nishant-ai/Snap2Caption/blob/main/training_scripts/code_wandb.ipynb) |

*Distributed training (DDPÂ + FP16) cut wallâ€‘clock from 8â€¯h â†’ 5â€¯h while BLEUâ€‘4 improved to 0.31.* Weekly CronJob retrains, or Prometheus fires an onâ€‘demand run when drift is detected.

---

## Shipping Intelligence â€”â€¯**Infrastructure & Continuous Delivery**  *(UnitsÂ 2Â &Â 3)*

All infrastructure is codified and repeatable:

| Layer             | Repository location                                                                                                              |
| ----------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| Terraform         | [https://github.com/nishant-ai/Snap2Caption/tree/main/tf](https://github.com/nishant-ai/Snap2Caption/tree/main/tf)               |
| Ansible           | [https://github.com/nishant-ai/Snap2Caption/tree/main/ansible](https://github.com/nishant-ai/Snap2Caption/tree/main/ansible)     |
| Kubernetes        | [https://github.com/nishant-ai/Snap2Caption/tree/main/k8s](https://github.com/nishant-ai/Snap2Caption/tree/main/k8s)             |
| CIâ€¯/â€¯CDâ€¯Workflows | [https://github.com/nishant-ai/Snap2Caption/tree/main/workflows](https://github.com/nishant-ai/Snap2Caption/tree/main/workflows) |

GitHubÂ Actions builds every commit; ArgoÂ CD syncs to **staging**. ArgoÂ Rollouts governs progressive promotion to **canary** and **prod**; models advance automatically after SLOs stay green for 24â€¯h. When dataâ€‘drift alerts post a GitHubÂ Release, the same pipeline reâ€‘trains and redeploys, closing the loop.

**Chameleon quickâ€‘start**

```bash
# provision GPU K8s + storage via Terraform
make chi-init && make chi-apply

# bootstrap Argo CD (Ansible playbook)
make argocd-bootstrap

# monitor rollout, then try the API
make argocd-watch &
python demo_request.py sample.jpg
```

---

## Serving the Magic â€”â€¯**Realâ€‘time Inference & Evaluation**  *(UnitsÂ 6Â &Â 7)*

Serving stack:

| Component              | Link                                                                                                                                                                 |
| ---------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Base API package       | [https://github.com/nishant-ai/Snap2Caption/tree/main/base\_api](https://github.com/nishant-ai/Snap2Caption/tree/main/base_api)                                      |
| FastAPI endpoint       | [https://github.com/nishant-ai/Snap2Caption/blob/main/base\_api/fastapi\_app.py](https://github.com/nishant-ai/Snap2Caption/blob/main/base_api/fastapi_app.py)       |
| Model invocation logic | [https://github.com/nishant-ai/Snap2Caption/blob/main/base\_api/model.py](https://github.com/nishant-ai/Snap2Caption/blob/main/base_api/model.py)                    |
| Feedback loop backend  | [https://github.com/nishant-ai/Snap2Caption/blob/main/base\_api/store\_feedback.py](https://github.com/nishant-ai/Snap2Caption/blob/main/base_api/store_feedback.py) |

The FastAPI server loads an 8â€‘bit ONNX checkpoint, decodes images with turboâ€‘JPEG, and answers:

```http
POST /generate-caption
{ "image_base64": "â€¦" }
â†’ 200 OK
{ "caption": "Golden hour coffee vibes â˜•ðŸŒ‡", "hashtags": ["#citysunset", â€¦] }
```

KEDA scales replicas 0â€¯â†’â€¯10 based on queue depth. Offline tests (`pytest`), load tests (`locust`) and Grafana dashboards validate quality and performance before promotion.

---

## Frontend Touchpoint

A lightweight React frontâ€‘end consumes the API and showcases caption suggestions:
[https://github.com/nishant-ai/Snap2Caption/tree/main/frontend](https://github.com/nishant-ai/Snap2Caption/tree/main/frontend)

---

## Repository Atlas

| Area                       | Real link                                                                                                                                       |
| -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| Infrastructure (Terraform) | [https://github.com/nishant-ai/Snap2Caption/tree/main/tf](https://github.com/nishant-ai/Snap2Caption/tree/main/tf)                              |
| Configuration (Ansible)    | [https://github.com/nishant-ai/Snap2Caption/tree/main/ansible](https://github.com/nishant-ai/Snap2Caption/tree/main/ansible)                    |
| K8s manifests              | [https://github.com/nishant-ai/Snap2Caption/tree/main/k8s](https://github.com/nishant-ai/Snap2Caption/tree/main/k8s)                            |
| CI/CD workflows            | [https://github.com/nishant-ai/Snap2Caption/tree/main/workflows](https://github.com/nishant-ai/Snap2Caption/tree/main/workflows)                |
| Model training scripts     | [https://github.com/nishant-ai/Snap2Caption/tree/main/training\_scripts](https://github.com/nishant-ai/Snap2Caption/tree/main/training_scripts) |
| Full training pipeline     | [https://github.com/nishant-ai/Snap2Caption/tree/training-pipeline](https://github.com/nishant-ai/Snap2Caption/tree/training-pipeline)          |
| Serving code               | [https://github.com/nishant-ai/Snap2Caption/tree/main/base\_api](https://github.com/nishant-ai/Snap2Caption/tree/main/base_api)                 |
| Frontend (React)           | [https://github.com/nishant-ai/Snap2Caption/tree/main/frontend](https://github.com/nishant-ai/Snap2Caption/tree/main/frontend)                  |

---

## Local Taste Test

```bash
make docker-build
make docker-run
python demo_request.py sample.jpg
```

---

## License

MIT
