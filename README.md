# Snap2Caption

> *Generate an Instagramâ€‘ready caption and a list of highâ€‘engagement hashtags from any photograph in a single API call.*

---

## 1â€¯Â·â€¯Who We Serve â€”Â Aisha *(Valueâ€¯Proposition)*

Aisha is a solo lifestyle creator who snaps urban photos during her commute. She wants to post consistently but hates spending time on copyâ€‘writing and keyword research. **Snap2Caption** lets her drop an image into a mobile or desktop workflow and receive:

* a short caption in her tone of voice
* 5â€‘10 contextâ€‘aware hashtags optimised for reach

Design promisesâ€”and how the code meets them:

| Need              | Implementation in the repo                                                                                                                               |
| ----------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Oneâ€‘step usage    | HTTP `POST /generate-caption` in the [FastAPI app](https://github.com/nishant-ai/Snap2Caption/blob/main/base_api/fastapi_app.py)                         |
| <â€¯2â€¯s P90 latency | Lightweight inference stack written inÂ Python; model cached on startâ€‘up; no heavy image preprocessing                                                    |
| Fresh hashtags    | Model can be reâ€‘trained offline with new data via scripts in [`training_scripts`](https://github.com/nishant-ai/Snap2Caption/tree/main/training_scripts) |

**Current scale** (MayÂ 2025)

| Item                         | Figure                |
| ---------------------------- | --------------------- |
| Training images used in demo | 100â€¯k (â‰ˆâ€¯5â€¯GB)        |
| Fineâ€‘tuned checkpoint        | 14â€¯GB FP16            |
| Observed peak load (staging) | \~300 requestsâ€¯/â€¯hour |

---

## 2â€¯Â·â€¯Data & Training *(Unitsâ€¯4,â€¯5Â &Â 8)*

All model development lives inside the **`training_scripts`** directory. It contains:

* `finetune_lora.py`Â â€“ LoRA fineâ€‘tuning driver for LLaVAâ€‘1.5â€‘7B
* `dataset_loader.py`Â â€“ minimal loader for InstaCitiesâ€‘style JPEG + caption rows
* `code_wandb.ipynb`Â â€“ interactive notebook that logs experiments toÂ WeightsÂ &Â Biases

Training is executed locally or on a GPU node with:

```bash
python training_scripts/finetune_lora.py \
  --config configs/finetune_a100.yaml
```

Artifacts are written to `output/` and can be copied into the serving container.

---

## ğŸ§  Unit 4 & 5: Model Training

### ğŸ¯ Modeling

We use **LLaVA models** for multimodal caption generation:

- [LLaVA-1.5 (7B, Vicuna backend)](https://huggingface.co/llava-hf/llava-1.5-7b-hf)
- [LLaVA-1.6 (7B, Mistral backend)](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf)

These models are fine-tuned using **LoRA adapters** for parameter-efficient adaptation on the **InstaCities1M** dataset, where each image is mapped to a caption.

- **Input**: RGB image (preprocessed to 224x224)  
- **Output**: Natural language caption (generated via autoregressive decoding)

ğŸ”— [training.py](https://github.com/nishant-ai/Snap2Caption/blob/training-pipeline/training.py)

---

### ğŸ—ï¸ Infrastructure: Training Environment (Docker)

To ensure portability and reproducibility, we define our full training environment in:

ğŸ”— [Dockerfile.trainingenv](https://github.com/nishant-ai/Snap2Caption/blob/training-pipeline/Dockerfile.trainingenv)

It includes:
- Python 3.10
- PyTorch 2.2.1 + CUDA 12.1
- MLflow, HuggingFace Hub, LoRA support
- Pillow, Lightning, Tensorboard, etc.

**Build the image:**

```bash
docker build -f Dockerfile.trainingenv -t snap2caption-train .
```

**Run training inside the container:**

```bash
docker run --rm --gpus all \
  -v /mnt/dataset:/mnt/dataset \
  -v $(pwd)/configs:/configs \
  -v $(pwd)/outputs:/app/outputs \
  snap2caption-train \
  python training.py --config /configs/config.yaml
```

---
### âš™ï¸ Configuration-Driven Pipeline

We use a dynamic YAML-based configuration system to control the training pipeline.

ğŸ”— [config.yaml](https://github.com/nishant-ai/Snap2Caption/blob/training-pipeline/config.yaml)  
ğŸ”— [commands.txt](https://github.com/nishant-ai/Snap2Caption/blob/training-pipeline/commands.txt)

- `config.yaml` defines:
  - Dataset paths and per-city sampling
  - LoRA hyperparameters (e.g., learning rate, rank, batch size)
  - MLflow logging and model registration behavior
  - Final evaluation and model output path

- `commands.txt` includes:
  - **All shell commands required to build the Docker container**
  - **Run the training pipeline end-to-end**
  - **Perform inference tests or trigger retraining runs**

> ğŸ’¡ This file serves as a **complete reference** for executing the pipeline on any fresh Chameleon node or local setup. Just follow the commands top to bottom.

---

### ğŸ”„ Train and Re-train Pipeline

- **Entry point**: [training.py](https://github.com/nishant-ai/Snap2Caption/blob/training-pipeline/training.py)
- Accepts dynamic config paths
- Fine-tunes LLaVA with LoRA
- Logs all metrics and artifacts to a **remote MLflow server** at KVM@TACC
- Supports retraining on feedback data (see Unit 8)

---
### ğŸ” Experiment Tracking with MLflow

All training runs are tracked using **MLflow**, hosted at our central node on KVM@TACC:

ğŸŒ MLflow UI: [http://129.114.25.254:8000](http://129.114.25.254:8000)

Logged details include:
- **Parameters**: learning rate, batch size, LoRA config
- **Metrics**: test loss, BLEU, CIDEr, token length
- **Artifacts**: LoRA adapters, sample captions
- **Conditional model registration**: based on evaluation performance

ğŸ”— [MLflow logging snippet (training.py#L340â€“370)](https://github.com/nishant-ai/Snap2Caption/blob/training-pipeline/training.py#L340)

---

## ğŸš€ Unit 6 & 7: Model Serving and Optimization

### ğŸ§© Model Inference Server (FastAPI-based)

This component is the **model inference server**, responsible for **serving the trained image captioning model via a production-grade FastAPI endpoint**.

It exposes a REST API where users can send an image and receive a caption in real time. This server is containerized and instrumented with monitoring hooks for production deployment.

- **API Input**: RGB image (base64-encoded JPEG or PNG)  
- **API Output**: A generated natural language caption in JSON format

The API also includes monitoring instrumentation via **Prometheus** and dashboards powered by **Grafana**, both hosted on our central KVM@TACC node.

> ğŸŒ **Monitoring Dashboard (Hosted at KVM@TACC)**  
> - **Floating IP**: `129.114.25.254`  
> - **Grafana Dashboard**: [http://129.114.25.254:3000/?orgId=1&from=now-6h&to=now&timezone=browser](http://129.114.25.254:3000/?orgId=1&from=now-6h&to=now&timezone=browser)  
> - **Prometheus Metrics UI**: [http://129.114.25.254:9090/query](http://129.114.25.254:9090/query)  
> - **FastAPI `/metrics` Endpoint**: [http://129.114.25.254:8000/metrics](http://129.114.25.254:8000/metrics)

**Main Serving Code**:

- ğŸ”— [`inference_server_setup.py`](https://github.com/nishant-ai/Snap2Caption/blob/model_serving_endpoint/inference_server_setup.py) â€“ Main FastAPI app with model loading and inference logic
- ğŸ”— [`llava_fastapi_server.py`](https://github.com/nishant-ai/Snap2Caption/blob/model_serving_endpoint/llava_fastapi_server.py) â€“ Optional wrapper/entry script for uvicorn
- ğŸ”— [`request.py`](https://github.com/nishant-ai/Snap2Caption/blob/model_serving_endpoint/request.py) â€“ Defines API request/response formats using Pydantic

Start the API:

```bash
uvicorn inference_server_setup:app --host 0.0.0.0 --port 8000
```

Access Swagger docs at: [http://localhost:8000/docs](http://localhost:8000/docs)

FastAPI inference endpoint:
http://129.114.109.50:8000/generate_caption

ğŸ”— Feedback handling API:
-[store_feedback.py](https://github.com/nishant-ai/Snap2Caption/blob/main/base_api/store_feedback.py)

Start the Feedback API:
```bash
source minio_config.sh && uvicorn store_feedback:app --host 0.0.0.0 --port 8010
```
Those feedback samples are then:

1.Annotated in Label Studio.

2.Used for retraining and fine-tuning the captioning model.


## ğŸ—„ï¸ Unit 8: Persistent Storage & Data Services

### ğŸ’¾ Centralized Block Storage at KVM@TACC

We provisioned a persistent **block storage volume** on our main node at **KVM@TACC**. This storage is mounted locally using `rclone` and is used to persist the runtime data of all critical MLOps services.

> ğŸ“ Mounted at: `/mnt/<folder_name>`  
> ğŸ”§ Mounted using: `rclone`  
> ğŸŒ Node: KVM@TACC (`129.114.25.254`)

#### â¬†ï¸ Push New Data to Block Storage

```bash
rclone copy <local_path> <remote>:<bucket_or_path>
```

Example:

```bash
rclone copy ./feedback/user123/feedback.json chameleon:mlops-data/feedback/user123/
```

#### Services Using Block Storage

| Service       | Description                          |
|---------------|--------------------------------------|
| MLflow        | Logs experiments, artifacts          |
| MinIO         | Hosts feedback + intermediate data   |
| Prometheus    | Stores metrics (latency, counts)     |
| Grafana       | Persists dashboards and settings     |
| PostgreSQL    | Backs MLflowâ€™s tracking DB           |
| Ray           | For distributed job execution (future)|

---

### ğŸª£ Object Storage at CHI@UC â€“ Dataset Hosting

For hosting the **training dataset (InstaCities1M)**, we provisioned and mounted **object storage** at the **CHI@UC** site.

- This allows high-throughput access during model training
- Reduces I/O load on the central KVM node
- Scales with storage and compute demand

> ğŸ“ Hosted on: CHI@UC  
> ğŸ“ Mounted via `rclone` or FUSE into the training node  
> ğŸ·ï¸ Mounted at paths like: `/mnt/images`, `/mnt/captions`

#### Example Mount Command

```bash
rclone mount chameleon:instacities1m/images /mnt/images --daemon
rclone mount chameleon:instacities1m/captions /mnt/captions --daemon
```

These paths are then passed into the training container:

```bash
docker run ... \
  --mount type=bind,source=/mnt/images,target=/mnt/InstaCities1M/images,readonly \
  --mount type=bind,source=/mnt/captions,target=/mnt/InstaCities1M/captions,readonly \
  ...
```

ğŸ”„ Data Pipeline (ETL)
The offline data pipeline is composed of three main stages:

ğŸ”¹ Extract
Downloads the InstaCities1M.zip dataset using aria2 for fast, parallelized download via multiple connections.

ğŸ”¹ Transform
Filters image-caption pairs for the following cities:
["newyork", "chicago", "sanfrancisco"]

Converts all folder names to lowercase for consistency.

Verifies and matches each image to a valid caption.

ğŸ”¹ Load
Uploads the cleaned and structured dataset to Chameleon Object Storage.

Container: object-persist-sbb9447-project54

Accessible at: [Chameleon UC Dashboard - Object Storage]("https://chi.uc.chameleoncloud.org/project/containers/container/object-persist-sbb9447-project54")

ğŸ“¦ Storage Volume
All dataset stages are persisted under the volume: instacities1m

ğŸ”— Code References
See the extract-data, transform-data, and load-data services in
[docker-compose.yml]("https://github.com/nishant-ai/Snap2Caption/blob/main/docker-compose.yml")

ğŸ§¹ Data Split & Preprocessing
Dataset was shuffled and split using an 80/10/10 ratio:

80% Training

10% Validation

10% Testing

Care was taken to avoid data leakage between partitions.

Preprocessing Steps:
Lowercased all folder and city names

Ensured one-to-one matching between images and captions

Removed corrupted or unmatched image-caption pairs
---

### âœ… Storage Strategy Summary

| Storage Type  | Location     | Purpose                                   |
|---------------|--------------|-------------------------------------------|
| Block Storage | KVM@TACC     | Persistent logs, system state, feedback   |
| Object Storage| CHI@UC       | Training datasets (images + captions)     |

This hybrid setup ensures:
- Efficient I/O for training
- Secure persistence of all service-level data
- Separation of concerns between system and training workloads


## 4â€¯Â·â€¯Infrastructure as Code *(Unitsâ€¯2Â &â€¯3)*

| Layer          | Description                                               | Directory                                                                     |
| -------------- | --------------------------------------------------------- | ----------------------------------------------------------------------------- |
| Terraform      | Allocates GPU VM, VPC, and S3Â bucket on Chameleon         | [`tf`](https://github.com/nishant-ai/Snap2Caption/tree/main/tf)               |
| Ansible        | Installs Docker, NVIDIA drivers, and pulls runtime images | [`ansible`](https://github.com/nishant-ai/Snap2Caption/tree/main/ansible)     |
| Kubernetes     | Optional manifests for container orchestration            | [`k8s`](https://github.com/nishant-ai/Snap2Caption/tree/main/k8s)             |
| GitHubÂ Actions | CI pipeline for build, push, and lint                     | [`workflows`](https://github.com/nishant-ai/Snap2Caption/tree/main/workflows) |

### Chameleon oneâ€‘shot deployment

```bash
# 1Â Â·Â Provision resources
cd tf && terraform init && terraform apply -auto-approve

# 2Â Â·Â Configure VM
ansible-playbook ansible/site.yml -i ansible/inventory

# 3Â Â·Â Run serving container
ssh <vm_ip>
docker compose -f base_api/docker-compose.yml up -d
```
ğŸ—ï¸ Infrastructure Provisioning with Terraform
Terraform was used to provision the required infrastructure on Chameleon Cloud, which included:

Three Virtual Machines (VMs) for a self-managed Kubernetes cluster.

Network configurations to allow secure communication between nodes.

ğŸ”§ Configuration with Ansible
After provisioning, Ansible was utilized for configuration and package installations. Key tasks included:

SSH configuration and key management.

Installing dependencies like Docker and Kubernetes packages.

Running Ansible playbooks to configure the VMs for Kubernetes deployment.

â˜¸ï¸ Kubernetes Deployment
Kubernetes was deployed using Kubespray, an Ansible-based Kubernetes installer. This allowed for:

Setting up a three-node Kubernetes cluster.

Applying essential configurations like Docker networking and registry.

Deploying the cluster in a resilient, highly-available configuration.

ğŸ’¡ Key Takeaways & Issues Faced
The primary takeaways from this setup included:

Understanding how to manage cloud infrastructure using Terraform and Ansible.

Automating complex multi-node Kubernetes deployments with Kubespray.

Issues faced included:

Firewall configurations.

SSH permission adjustments.

These were resolved by updating Ansible playbooks.

ğŸ”„ Next Steps
Debug the issues in the Argo Workflows and finalize the CI/CD pipelines.

Extend the deployment to include ArgoCD for continuous delivery.

Optimize the resource allocation for better scalability and cost-effectiveness.

---

## 5â€¯Â·â€¯Frontend Preview

A React demo lives in [`frontend`](https://github.com/nishant-ai/Snap2Caption/tree/main/frontend). Point it at the FastAPI host to try caption generation in the browser.

---

## 6â€¯Â·â€¯Repository Map (quick reference)

| Area                        | Path                 |
| --------------------------- | -------------------- |
| Model serving (FastAPI)     | `base_api/`          |
| Training scripts            | `training_scripts/`  |
| InfrastructureÂ â€“Â Terraform  | `tf/`                |
| InfrastructureÂ â€“Â Ansible    | `ansible/`           |
| InfrastructureÂ â€“Â Kubernetes | `k8s/`               |
| CI/CD workflows             | `.github/workflows/` |
| React frontâ€‘end             | `frontend/`          |

---

## 7â€¯Â·â€¯Run a Local Demo

```bash
# build image
make docker-build
# start API on localhost:8000
make docker-run
# call it
python demo_request.py sample.jpg
```

---

## License

MIT
