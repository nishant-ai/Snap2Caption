# Dockerfile for FastAPI-based LLaVA+LoRA server (base64 JSON input version)
FROM python:3.10-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    build-essential \
    libglib2.0-0 libsm6 libxrender-dev libxext6 \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN pip install --upgrade pip

# Install PyTorch with CUDA 11.8 support
RUN pip install torch==2.1.0 torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/cu118

# Install Hugging Face + FastAPI dependencies
RUN pip install \
    "numpy<2.0" \
    transformers \
    peft \
    accelerate \
    omegaconf \
    safetensors \
    fastapi \
    uvicorn \
    pillow \
    typing_extensions \
    python-multipart \
    prometheus_client

# Set working directory
WORKDIR /app

# Copy server code
COPY llava_fastapi_server.py .

# Expose FastAPI port
EXPOSE 8000

# Start the FastAPI app
CMD ["uvicorn", "llava_fastapi_server:app", "--host", "0.0.0.0", "--port", "8000"]